# Task 5: Creating a Golden Test Data Set

**You are an AI Evaluation & Performance Engineer.**  The AI Systems Engineer who built the initial RAG system has asked for your help and expertise in creating a "Golden Data Set" for evaluation.

<aside>
üìù

Task 5: Generate a synthetic test data set to baseline an initial evaluation with RAGAS

</aside>

**‚úÖ¬†Deliverables**

1. Assess your pipeline using the RAGAS framework including key metrics faithfulness, response relevance, context precision, and context recall.  Provide a table of your output results.

Questions generated by RAGAS:
What is the claim amount for CLM-001?
Which claims were filed in January 2024?

============================================================
üìä RAGAS EVALUATION RESULTS
============================================================

           Metric Score Percentage          Grade
     Faithfulness 0.200      20.0% D (Needs Work)
 Answer Relevancy 0.987      98.7% A+ (Excellent)
Context Precision 0.000       0.0% D (Needs Work)
   Context Recall 0.000       0.0% D (Needs Work)
          OVERALL 0.297      29.7% D (Needs Work)

2. What conclusions can you draw about the performance and effectiveness of your pipeline with this information?

    1. Faithfulness (20.0%)
    ‚ö†Ô∏è Answers may contain hallucinations
    
    2. Answer Relevancy (98.7%)
    ‚úÖ Answers are relevant to questions

    3. Context Precision (0.0%)
    ‚ö†Ô∏è Retrieved chunks contain irrelevant information
    
    4. Context Recall (0.0%)
    ‚ö†Ô∏è Missing important information in retrieval


    Dense vector retrieval or the Naive retriever gave me very low scores on all the metrics measured above.